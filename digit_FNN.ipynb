{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition with Neural Networks\n",
    "\n",
    "This notebook demonstrates how to build and train a neural network for digit recognition using Tensorflow, Keras, Pandas and Numpy.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "To set up the environment, please follow these steps:\n",
    "\n",
    "0. open terminal and cd your way to the folder containing the jupiter notebook.\n",
    "\n",
    "1. create a virtual environment using Python's built-in venv module:\n",
    "\n",
    "    *python -m venv venv*\n",
    "\n",
    "2. Activate the virtual environment:\n",
    "\n",
    "    for windows:\n",
    "\n",
    "    *venv\\Scripts\\activate*\n",
    "\n",
    "    for macOS/Linux:\n",
    "    \n",
    "    *source venv/bin/activate*\n",
    "\n",
    "3. Install dependencies\n",
    "\n",
    "    write:\n",
    "\n",
    "    *pip install -r requirements.txt*\n",
    "\n",
    "    The requirements.txt file includes all the necessary dependencies for the project,\n",
    "\n",
    "If you encounter any issues with the setup, try updating pip and the installed packages:\n",
    "\n",
    "*pip install --upgrade pip*\n",
    "\n",
    "*pip install --upgrade -r requirements.txt*\n",
    "\n",
    "If the save_canvas_image function does not work and you can't draw on the canvas using your right click, try running the notebook on a browser or a different kernel.\n",
    "You might also skip the canvas image function and draw a handwritten digit using editing software like paint. \n",
    "Save the digit image in the same folder as your notebook and specify the digit's path. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical computations and handling arrays\n",
    "import pandas as pd  # For data manipulation (optional, depending on use)\n",
    "import matplotlib.pyplot as plt  # For plotting and visualizing data\n",
    "from tensorflow.keras.datasets import mnist  # To load the MNIST dataset\n",
    "from tensorflow.keras.utils import to_categorical  # For one-hot encoding of labels\n",
    "from tensorflow.keras.models import Sequential  # To build a sequential model\n",
    "from tensorflow.keras.layers import Dense  # To add dense layers to the model\n",
    "from tensorflow.keras.optimizers import Adam  # For the optimizer used in training\n",
    "from PIL import Image  # To handle images (useful for user input)\n",
    "import cv2  # For image processing tasks\n",
    "from ipycanvas import Canvas # For drawing on a canvas\n",
    "from IPython.display import display # For displaying the canvas\n",
    "from ipywidgets import IntSlider \n",
    "from PIL import Image, ImageFilter # For image processing tasks\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Environment setup successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Display the shapes of the loaded data\n",
    "print(f'Training data shape: {x_train.shape}, Training labels shape: {y_train.shape}')\n",
    "print(f'Testing data shape: {x_test.shape}, Testing labels shape: {y_test.shape}')\n",
    "\n",
    "# Display a few samples of the data\n",
    "plt.figure(figsize=(10, 5)) # create a new figure for plotting with a specified size of 10x5 inches.\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1) # create a 2x5 grid of subplots and selects the (i+1)th subplot.\n",
    "    plt.imshow(x_train[i], cmap='gray')  # Display the image in grayscale\n",
    "    plt.title(f'Label: {y_train[i]}') # Set the title of the subplot to the corresponding label\n",
    "    plt.axis('off') # Turn off the axes for the subplot for clean display\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the MNIST dataset, the training data shape refers to the dimensions of the **x_train array**, which contains the training images. The MNIST dataset consists of 28x28 pixel grayscale images of handwritten digits (0-9).\n",
    "\n",
    "When you load the MNIST data using **mnist.load_data()**, the x_train array typically has the shape **(60000, 28, 28)**, meaning:\n",
    "\n",
    "60000: The number of training images.\n",
    "28: The height of each image in pixels.\n",
    "28: The width of each image in pixels.\n",
    "So, x_train.shape would output (60000, 28, 28), indicating that there are 60,000 training images, each of size 28x28 pixels.\n",
    "\n",
    "Similarly, **y_train.shape** would typically be (60000,), indicating that there are 60,000 labels corresponding to the 60,000 training images. Each label is a single integer representing the digit (0-9) shown in the corresponding image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x set (Features/Input Data):**\n",
    "This set contains the input data that the model will use to learn patterns. In the MNIST dataset, x_train contains the images of handwritten digits.\n",
    "\n",
    "**y set (Labels/Output Data):**\n",
    "This set contains the labels or the target values that correspond to the input data. In the MNIST dataset, y_train contains the digit labels (0-9) for each image in x_train.\n",
    "\n",
    " The model uses the **x set** to learn and the **y set** to evaluate its performance.\n",
    "\n",
    "\n",
    "The reason that we have 4 datasets **X_train, Y_train, X_test, Y_test** is to train the model on one set, and then test the model on new unseen data (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential() #nitialize a sequential model, which means the layers are stacked in a linear way.\n",
    "\n",
    "# Input layer with 784 neurons (28*28 pixels flattened)\n",
    "# First hidden layer with 128 neurons and ReLU activation\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Second hidden layer with 64 neurons and ReLU activation\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one for each digit) and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential model is a linear stack of layers, where each layer has exactly one input tensor and one output tensor. This model is best suited for building simple neural networks where layers are added one after another.\n",
    "\n",
    "The Sequential model is an object from the **tensorflow.keras.models module**, part of the **Keras API** integrated within TensorFlow.\n",
    "\n",
    "**Dense** refers to a densely connected neural network layer, and is a type of layer in a neural network where **each neuron is connected to every neuron in the previous layer**.\n",
    "\n",
    "Function: The **model.add** method is used to add layers to the model. In a Sequential model, layers are added one after another in a linear stack.\n",
    "Usage: Each call to model.add appends a new layer to the model.\n",
    "\n",
    "**Softmax** is specifically designed for multi-class classification problems where only one output class can be correct at a time.\n",
    "\n",
    "**ReLU** outputs zero for all negative inputs, effectively \"turning off\" neurons that donâ€™t contribute positively, leading to sparse representations that can improve efficiency.\n",
    "Hidden layers are responsible for transforming the input data into more abstract and useful representations. ReLU enables the network to learn complex, non-linear patterns by introducing non-linearity, which is crucial for the model's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling** the model sets up the rules for how the model will learn and be evaluated.\n",
    "\n",
    "**optimizer='adam':** Specifies how the model updates its weights based on the error calculated by the loss function. Adam is efficient, adaptive, and widely used.\n",
    "\n",
    "**loss='categorical_crossentropy':** Measures how well the model's predicted probabilities align with the true labels, crucial for multi-class classification tasks.\n",
    "\n",
    "**metrics=['accuracy']:** Provides a way to monitor how well the model is performing during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels by converting them to one-hot encoded format\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train.reshape(-1, 784),  # Reshape training images to 784 (28x28 flattened)\n",
    "    y_train_one_hot,           # Use one-hot encoded labels\n",
    "    epochs=15,                 # Number of times the model will see the entire training set\n",
    "    batch_size=64,             # Number of samples per gradient update\n",
    "    validation_split=0.2,      # Use 20% of training data for validation to monitor performance\n",
    "    verbose=1                  # Print progress during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**to_categorical(y_train)** and **to_categorical(y_test)** are converted into a binary vector with the length equal to the number of classes, 10 numbers. if the label is \"3\", the vector will look like this: **[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]**\n",
    "\n",
    "**x_train.reshape(-1, 784)** Reshapes the training images into a 2D array where each image is a single row of 784 pixels. The -1 automatically adjusts the first dimension to match the number of training samples.\n",
    "\n",
    "**y_train_one_hot** Uses the one-hot encoded labels for training, which match the softmax output format from the model.\n",
    "\n",
    "**epochs=10** Specifies the number of times the model will iterate over the entire training dataset. More epochs allow the model to learn better, but too many can lead to overfitting.\n",
    "\n",
    "**batch_size=32** Determines how many training samples are used per gradient update. A smaller batch size allows for quicker updates but can lead to noisier training, while a larger batch size is more stable but slower.\n",
    "\n",
    "**validation_split=0.2**: Allocates 20% of the training data for validation. This helps you monitor the model's performance on unseen data during training, providing a sense of whether it is overfitting.\n",
    "\n",
    "**verbose=1**: Controls the verbosity of the training output. A setting of 1 provides detailed logs of training progress, while 0 would suppress this output.\n",
    "\n",
    "The **model.fit()** function returns a history object, which stores details about the training process, including the loss and accuracy over each epoch for both training and validation data. This can be useful for plotting learning curves and evaluating model performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**history.history['accuracy']** Extracts the training accuracy values for each epoch from the history object.\n",
    "\n",
    "**history.history['val_accuracy']** Extracts the validation accuracy values for each epoch (the 20% of the dataset that wac allocated by the validation split).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test.reshape(-1, 784), y_test_one_hot, verbose=1)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_test.reshape(-1, 784)** reshapes test images to flattened (required for evaluation)\n",
    "\n",
    "**verbose=1** Prints the evaluation progress as its set to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a canvas for drawing\n",
    "canvas = Canvas(width=200, height=200, background_color='black')  # Larger canvas for easier drawing\n",
    "display(canvas)\n",
    "\n",
    "# Function to save the drawing as an image\n",
    "def save_canvas_image(canvas, save_path):\n",
    "    \"\"\"\n",
    "    Saves the current drawing on the canvas to an image file.\n",
    "\n",
    "    Parameters:\n",
    "    - canvas: ipycanvas.Canvas, the drawing canvas\n",
    "    - save_path: str, the file path where the image will be saved\n",
    "    \"\"\"\n",
    "    # Save the current drawing to a PNG file\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(canvas.to_filelike_object().getvalue())\n",
    "    print(f'Image saved to {save_path}')\n",
    "\n",
    "# Instructions for the user\n",
    "print(\"Draw a digit on the canvas. Use the left mouse button to draw.\")\n",
    "print(\"When finished, run the cell below to save and predict the digit.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cant draw on the white canvas. Try to use open the jupiter notebook on a browser.\n",
    "You might also skip the following cell and draw a digit in paint with black background and white digit. \n",
    "Use the preprocessing and predict user image functions with your paint digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas_digit_path = 'path/to/save/digit.png'  # File path to save the drawn digit\n",
    "save_canvas_image(canvas, canvas_digit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image to match the format used in training the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image file.\n",
    "    - is_user_image: bool, whether the image is user-drawn (potentially inverted colors).\n",
    "\n",
    "    Returns:\n",
    "    - preprocessed_image: numpy array, formatted for prediction.\n",
    "    \"\"\"\n",
    "    # Load, convert to grayscale, and resize\n",
    "    img = Image.open(image_path).convert('L')  # 'L' mode is for grayscale\n",
    "    img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "\n",
    "\n",
    "    # Convert to array without normalizing\n",
    "    img_array = np.array(img).astype('float32')  # Keep values between 0 and 255\n",
    "    print(\"Preprocessed image details:\")\n",
    "    print(f\"Shape: {img_array.shape}, Min: {img_array.min()}, Max: {img_array.max()}\")\n",
    "\n",
    "    # Flatten to match input shape (1, 784)\n",
    "    preprocessed_image = img_array.flatten().reshape(1, 784)\n",
    "\n",
    "    # Display the processed image\n",
    "    plt.imshow(img_array, cmap='gray')\n",
    "    plt.title('Processed Test Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return preprocessed_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_images(model, preprocessed_image1, preprocessed_image2):\n",
    "    \"\"\"\n",
    "    Predicts the digits from two preprocessed images using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained Keras model.\n",
    "    - preprocessed_image1: numpy array, first processed image ready for prediction (user image).\n",
    "    - preprocessed_image2: numpy array, second processed image ready for prediction (additional image).\n",
    "\n",
    "    Returns:\n",
    "    - predicted_digit1: int, the digit predicted by the model for the first image.\n",
    "    - predicted_digit2: int, the digit predicted by the model for the second image.\n",
    "    \"\"\"\n",
    "    # Predict the first image\n",
    "    prediction1 = model.predict(preprocessed_image1)\n",
    "    predicted_digit1 = np.argmax(prediction1)\n",
    "    \n",
    "    # Print results for the first image\n",
    "    print(f'Prediction Probabilities for the user digit: {prediction1}')\n",
    "    print(f'Predicted Digit for the user digit: {predicted_digit1}')\n",
    "    \n",
    "    # Predict the second image\n",
    "    prediction2 = model.predict(preprocessed_image2)\n",
    "    predicted_digit2 = np.argmax(prediction2)\n",
    "    \n",
    "    # Print results for the second image\n",
    "    print(f'Prediction Probabilities for the digit from dataset: {prediction2}')\n",
    "    print(f'Predicted Digit for the dataset digit: {predicted_digit2}')\n",
    "    \n",
    "    return predicted_digit1, predicted_digit2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the user-provided handwritten digit image\n",
    "image_path = 'path/to/user/image.png'\n",
    "processed_user_image = preprocess_image(image_path)\n",
    "\n",
    "random_image = x_train[np.random.randint(x_train.shape[0])]\n",
    "random_image_path = 'path/to/random/digit.png'  # Save the random training set image\n",
    "Image.fromarray(random_image).save(random_image_path)\n",
    "processed_dataset_image = preprocess_image(random_image_path) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_digit_user = model.predict(processed_user_image).argmax()\n",
    "\n",
    "predicted_digit_dataset = model.predict(processed_dataset_image).argmax()\n",
    "\n",
    "print(f'Predicted Digit for User drawn digit: {predicted_digit_user}')\n",
    "print(f'Predicted Digit for Training Dataset digit: {predicted_digit_dataset}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
